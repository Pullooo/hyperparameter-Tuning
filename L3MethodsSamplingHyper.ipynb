{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bef4d8fd-1852-43c6-8e2c-ff5c4f90cda1",
   "metadata": {},
   "source": [
    "**L3 BASIC SEARCH ALGORITHMS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a008bc9-1426-450b-8e2a-cf83c2b17428",
   "metadata": {},
   "source": [
    "- The process of finding the best hyperparameters for a given dataset is called hyperparameter Optimization or Hyperparameter Tuning\n",
    "- The best hyperparameters are those that maximise the performance of the ML algorithm\n",
    "\n",
    "<p> A search consists of:</p>\n",
    "<p> - Hyperparameter space\n",
    "<p> - <b>A method for sampling candidate hyperparameters (the focus of this section)</b>\n",
    "<p> - A cross validation scheme\n",
    "<p> - A performance metric to minimise (or maximise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d23206a-b0aa-4c75-899d-7b0702de6355",
   "metadata": {},
   "source": [
    "**3.1 Hyperparameter tuning challenges**\n",
    "- we can't defind a formula to find hyperparameters\n",
    "- We try different combinations of hyperparameters and evaluate model performance\n",
    "- The critical steps is to choose how many different <b>hyperparameter combinations</b> we are going to set\n",
    "- Computer resources available to us\n",
    "\n",
    "<img src='challenges.png'>\n",
    "- Low effective dimensions\n",
    "<img src='dimensions.png' width=\"1300\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b809e5-f149-4add-8273-dea82d17b2eb",
   "metadata": {},
   "source": [
    "**Explanation of low effective dimensions**\n",
    "\n",
    "When decising what hyperparameters to use in tuning the model, it is important to understand that not every hyperparameter has the same impact on model performance:\n",
    "<p> <i>i. The min samples to split the nodes has a very small impact on the model performance vs number of trees and tree depth\n",
    "<p> <i>ii. Even with features that have a high impact on model performance e.g. Tree depth and number of trees, there's a point (e.g. numb of trees = 50, or tree depth =3 in our example) beyond which increasing that feature wont increase model performance\n",
    "<p> <i> iii. In the charts, it's more important to search hyper parameters in certain regions e.g. Tree depth: between 1 - 3, number of trees: between 10 - 50  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a69da0-a3f7-4a87-b7be-6a14155b22a3",
   "metadata": {},
   "source": [
    "**Basic Hyperparameter Tuning Methods**\n",
    "1. Manual Search\n",
    "2. Grid Search\n",
    "3. Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90ef0a6-a026-42fd-868f-87b5787f1fee",
   "metadata": {},
   "source": [
    "**1. Manual Search**\n",
    "\n",
    "Consists of trying and testing different hyperparameters manually.\n",
    "\n",
    "**Uses:**\n",
    "<p><i> i. To identify regions of promising hyperparameters</i>: Remember we said there are values ranges of the hyperparameter where increasing the value changes the performance significantly (see low effective dimensions above), while increasing beyond that range doesn't change performance much. Manual search informs us of those value ranges where the model doesn't increase it's performance further\n",
    "<p><i> ii. To delimit the Grid Search: </i> To run grid search, we need to create a hyperparameter space consisting of the interval of values we want to test. Usually these intervals are defined manually\n",
    "<p><i> iii. To get familiar with the hyperparameters and their effect on the models: </i>After manually changing the values of these parameters and seeing their impact on model performance, we begin to understand which ones have a greater impact on model performance and which ones dont.\n",
    "<p><i> iv. To establish the benchmark model: </i>Usually a quick model we build with our data after a little bit of data analysis. It is later optimised\n",
    "    \n",
    "**Limitations:**\n",
    "<p><i> i. Lack of reproducibility: </i> because we are testing manually, if another experiment tries different values, they may not arrive at the same conclusions that we did\n",
    "<p><i> ii. Time consuming: </i>\n",
    "<p><i> ii. Does not explore the entire hyperparameter space: </i>\n",
    "<p><i> ii. Does not scale: </i>\n",
    "    \n",
    "**1.1 Demo: Manual Search for Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "824e1b66-a5b5-4434-a4e3-081ec0bc3ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d5e89e6-2ece-4bd1-ba0a-fa7ca8370109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1       2       3        4        5       6        7       8   \\\n",
       "0  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710  0.2419   \n",
       "1  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869  0.07017  0.1812   \n",
       "2  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974  0.12790  0.2069   \n",
       "3  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520  0.2597   \n",
       "4  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980  0.10430  0.1809   \n",
       "\n",
       "        9   ...     20     21      22      23      24      25      26      27  \\\n",
       "0  0.07871  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
       "1  0.05667  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
       "2  0.05999  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
       "3  0.09744  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
       "4  0.05883  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
       "\n",
       "       28       29  \n",
       "0  0.4601  0.11890  \n",
       "1  0.2750  0.08902  \n",
       "2  0.3613  0.08758  \n",
       "3  0.6638  0.17300  \n",
       "4  0.2364  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)\n",
    "X = pd.DataFrame(breast_cancer_X)\n",
    "y = pd.Series(breast_cancer_y).map({0:1, 1:0})\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be5fdfff-dc72-47c6-b75f-e92718bb140e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.627417\n",
       "1    0.372583\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of benign () and malign tumors (1)\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "554d5dda-a03f-4bc9-ae3d-4669a52cebd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((398, 30), (171, 30))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data into train and validation split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d60e0e7-b911-4378-9739-9bf88d1522e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "**1.1 Manual search - Logistic Regression**\n",
    "- We'll first play with c = 0.001 vs 1: We know this is what affects the performance of logistic regression models the most\n",
    "- Then we'll play with regularization/penalty = l1 vs l2: This doesn't impact performance as much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "020a8e5a-ddbc-4818-aae0-0f4e1fd8a4b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean train set accuracy:  0.9170836537134519  +-  0.0038064240947020133\n",
      "mean test set accuracy:  0.9195886075949368  +-  0.006259426475686005\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "logit = LogisticRegression(\n",
    "    penalty='l2', C=0.001, solver='liblinear', random_state=4, max_iter=10000)\n",
    "\n",
    "# K-Fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=4)\n",
    "\n",
    "# estimate generalization error\n",
    "clf = cross_validate(\n",
    "    logit,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    scoring='accuracy', #we optimise the accuracy\n",
    "    return_train_score=True,\n",
    "    cv=kf\n",
    ")\n",
    "\n",
    "print('mean train set accuracy: ', np.mean(clf['train_score']), ' +- ', np.std(clf['train_score']))\n",
    "print('mean test set accuracy: ', np.mean(clf['test_score']), ' +- ', np.std(clf['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6d39f2-eb05-462e-98f8-aaba778c0fb0",
   "metadata": {},
   "source": [
    "**1.2 What if we didn't use hyperparameter at all, how will a base model perform?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35e802a0-5adc-4836-b774-17b9114a9568",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9170854271356784\n",
      "Test Accuracy:  0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "logit.fit(X_train, y_train)\n",
    "\n",
    "train_preds = logit.predict(X_train)\n",
    "test_preds = logit.predict(X_test)\n",
    "\n",
    "print('Train Accuracy: ', accuracy_score(y_train, train_preds))\n",
    "print('Test Accuracy: ', accuracy_score(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd3c1e-df43-4350-871d-ed074651ff2c",
   "metadata": {},
   "source": [
    "**1.2 Observations:**\n",
    "- The model base model (trained without Kfold cross validation) is surprisingly higher than the one trained through kfold_cross validation\n",
    "- Let's see if we can improve the model performance obtained through kfold cross validation (this time will use c =1 and retrain Kfold model)\n",
    "\n",
    "**Try Kfold with c=1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b5c196f-80d3-4306-8cc5-9577acac38f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean train set accuracy:  0.9604266477395951  +-  0.0015497668615040091\n",
      "mean test set accuracy:  0.9447784810126582  +-  0.02565126706427742\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "logit = LogisticRegression(\n",
    "    penalty='l2', C=1, solver='liblinear', random_state=4, max_iter=10000)\n",
    "\n",
    "# K-Fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=4)\n",
    "\n",
    "# estimate generalization error\n",
    "clf = cross_validate(\n",
    "    logit,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    scoring='accuracy', #we optimise the accuracy\n",
    "    return_train_score=True,\n",
    "    cv=kf\n",
    ")\n",
    "\n",
    "print('mean train set accuracy: ', np.mean(clf['train_score']), ' +- ', np.std(clf['train_score']))\n",
    "print('mean test set accuracy: ', np.mean(clf['test_score']), ' +- ', np.std(clf['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb65f1b-cdc2-442c-959a-59349a29e841",
   "metadata": {
    "tags": []
   },
   "source": [
    "**1.3 Observations**\n",
    "- We notice that the accuracy of the test set increased quite a bit, from 91 to 94%. However, the error also increased from 0.0063 to 0.026.\n",
    "- So let's try another value for c and see if the accuracy increases and error decreases\n",
    "\n",
    "**Model train with Kfold cv and c=0.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81fc2fb1-3cad-43fb-a385-9680ed5a5d52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean train set accuracy:  0.9484966779046153  +-  0.006111315451121751\n",
      "mean test set accuracy:  0.9347468354430379  +-  0.019811643085077286\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "logit = LogisticRegression(\n",
    "    penalty='l2', C=0.1, solver='liblinear', random_state=4, max_iter=10000)\n",
    "\n",
    "# K-Fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=4)\n",
    "\n",
    "# estimate generalization error\n",
    "clf = cross_validate(\n",
    "    logit,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    scoring='accuracy', #we optimise the accuracy\n",
    "    return_train_score=True,\n",
    "    cv=kf\n",
    ")\n",
    "\n",
    "print('mean train set accuracy: ', np.mean(clf['train_score']), ' +- ', np.std(clf['train_score']))\n",
    "print('mean test set accuracy: ', np.mean(clf['test_score']), ' +- ', np.std(clf['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a980ad-88c4-4670-a081-d155e1ea573f",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- Accuracy is better and error is much smaller.\n",
    "- Lets now see what the performance of a logistic regression model is with c=0.1 too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09ceda50-538b-4dbe-a9f6-5cb06d84af96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9447236180904522\n",
      "Test Accuracy:  0.9532163742690059\n"
     ]
    }
   ],
   "source": [
    "logit.fit(X_train, y_train)\n",
    "\n",
    "train_preds = logit.predict(X_train)\n",
    "test_preds = logit.predict(X_test)\n",
    "\n",
    "print('Train Accuracy: ', accuracy_score(y_train, train_preds))\n",
    "print('Test Accuracy: ', accuracy_score(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb5c980-2392-4f71-878f-962560f59dc0",
   "metadata": {},
   "source": [
    "**observations:**\n",
    "- Performance in test set is higher and that in train set is similar  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453fed0c-fdbf-4b50-8323-c0b337151eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelling",
   "language": "python",
   "name": "modelling_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
